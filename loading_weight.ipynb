{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from tensorflow.keras.utils import image_dataset_from_directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import keras\n",
    "from keras.layers import Dense, Flatten, Normalization, Dropout, Conv2D, MaxPooling2D, RandomFlip, RandomRotation, RandomZoom, BatchNormalization, Activation, AveragePooling2D\n",
    "from keras.models import Sequential\n",
    "from keras.losses import SparseCategoricalCrossentropy, CategoricalCrossentropy\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.utils import np_utils\n",
    "from keras import utils\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Input\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wget\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bar_custom(current, total, width=80):\n",
    "    print(\"Downloading: %d%% [%d / %d] bytes\" % (current / total * 100, current, total))\n",
    "\n",
    "zip_name = \"train.zip\"\n",
    "\n",
    "url = \"https://jrssbcrsefilesnait.blob.core.windows.net/3950data1/vegetable_image_dataset.zip\"\n",
    "\n",
    "if not os.path.exists(zip_name):\n",
    "    wget.download(url, zip_name, bar=None)\n",
    "\n",
    "with zipfile.ZipFile(zip_name, 'r') as zip_ref:\n",
    "    zip_ref.extractall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE=(224,224)\n",
    "batch_size = 16\n",
    "\n",
    "test_dir='Vegetable Images/test'\n",
    "test_ds = image_dataset_from_directory(\n",
    "    test_dir,\n",
    "    label_mode='categorical',\n",
    "    image_size = IMAGE_SIZE,\n",
    "    batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_url = 'http://'\n",
    "file_path = 'custom_weights.h5'\n",
    "\n",
    "response = requests.get(model_url)\n",
    "\n",
    "with open(file_path, 'wb') as file:\n",
    "    file.write(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "input_shape = (224, 224, 3)\n",
    "\n",
    "custom_model = Sequential([\n",
    "                    Conv2D(128, (3, 3), input_shape=input_shape, padding=\"same\", kernel_regularizer=\"l2\"),\n",
    "                    BatchNormalization(),\n",
    "                    Activation(\"relu\"),\n",
    "                    MaxPooling2D((2, 2)),\n",
    "                    Conv2D(128, (3, 3), padding=\"same\", kernel_regularizer=\"l2\"),\n",
    "                    Activation(\"relu\"),\n",
    "                    BatchNormalization(),\n",
    "                    MaxPooling2D((2, 2)),\n",
    "                    Dropout(.4),\n",
    "                    Conv2D(128, (3, 3), padding=\"same\", kernel_regularizer=\"l2\"),\n",
    "                    Activation(\"relu\"),\n",
    "                    BatchNormalization(),\n",
    "                    MaxPooling2D((2, 2)),\n",
    "                    Dropout(.4),\n",
    "                    Flatten(),\n",
    "                    Dense(128, activation='relu', kernel_regularizer=\"l2\"),\n",
    "                    Dense(15, activation=\"softmax\")])\n",
    "\n",
    "# use this variable to train the model or to load the model in from weights\n",
    "custom_trainable = False\n",
    "\n",
    "# Set the 'trainable' attribute to False\n",
    "for layer in custom_model.layers:\n",
    "    layer.trainable = custom_trainable\n",
    "\n",
    "custom_model.build(input_shape=(None, input_shape))\n",
    "custom_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_model.load_weights('custom_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "for images, labels in test_ds:\n",
    "    predictions = custom_model.predict(images)\n",
    "    y_true.extend(np.argmax(labels.numpy(), axis=-1))\n",
    "    y_pred.extend(np.argmax(predictions, axis=-1))\n",
    "y_true = np.array(y_true)\n",
    "y_pred = np.array(y_pred)\n",
    "\n",
    "\n",
    "conf_mat = confusion_matrix(y_true, y_pred)\n",
    "sns.heatmap(conf_mat, annot=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "backup",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
